






[{"content":" Introduction # Transformers have revolutionized Natural Language Processing\u0026hellip;\nThe Attention Mechanism # The key innovation is the self-attention mechanism. The formula is:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere $Q$ (Query), $K$ (Key), and $V$ (Value) are learned linear projections of the input embeddings.\nCode Example (Python) # import torch import torch.nn as nn # Simplified Self-Attention Example class SelfAttention(nn.Module): def __init__(self, embed_size, heads): super(SelfAttention, self).__init__() # ... (Implementation details) ... def forward(self, value, key, query, mask): # ... (Attention calculation) ... return out ","date":"12 May 2025","externalUrl":null,"permalink":"/posts/speculative-decoding/","section":"Posts","summary":"Explaining the core concepts behind the Transformer architecture, including self-attention.","title":"A Gentle Introduction to Transformer Models","type":"posts"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/","section":"Blowfish","summary":"","title":"Blowfish","type":"page"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/categories/concepts/","section":"Categories","summary":"","title":"Concepts","type":"categories"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning","type":"tags"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/series/nlp-basics/","section":"Series","summary":"","title":"NLP Basics","type":"series"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/transformers/","section":"Tags","summary":"","title":"Transformers","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/cnn/","section":"Tags","summary":"","title":"CNN","type":"tags"},{"content":" Project Goal # The objective was to implement a CNN from scratch\u0026hellip;\nDataset # Used the popular CIFAR-10 dataset containing 60,000 32x32 color images in 10 classes.\nModel Architecture # Designed a standard CNN with convolutional layers, ReLU activations, max-pooling, and fully connected layers.\n# Simplified PyTorch model definition import torch.nn as nn import torch.nn.functional as F class SimpleCNN(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) # ... more layers self.fc3 = nn.Linear(84, 10) def forward(self, x): # ... forward pass logic return x ","date":"20 March 2025","externalUrl":null,"permalink":"/projects/image-classification-cnn/","section":"My Projects","summary":"Built and trained a Convolutional Neural Network using PyTorch to classify images from the CIFAR-10 dataset.","title":"CNN for Image Classification (CIFAR-10)","type":"projects"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/portfolio/","section":"Tags","summary":"","title":"Portfolio","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/categories/projects/","section":"Categories","summary":"","title":"Projects","type":"categories"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"PyTorch","type":"tags"},{"content":"Hello! I\u0026rsquo;m [Your Name], a [Your Role] passionate about [Your Interests within ML/DS].\nThis blog serves as my digital notebook and portfolio where I share project write-ups, tutorials, and thoughts on machine learning concepts.\nYou can find my resume here (Place resume.pdf in the static/ folder).\nConnect with me via the links in the profile section or footer.\n","date":"1 January 2025","externalUrl":null,"permalink":"/about/","section":"Blowfish","summary":"\u003cp\u003eHello! I\u0026rsquo;m [Your Name], a [Your Role] passionate about [Your Interests within ML/DS].\u003c/p\u003e\n\u003cp\u003eThis blog serves as my digital notebook and portfolio where I share project write-ups, tutorials, and thoughts on machine learning concepts.\u003c/p\u003e","title":"About Me","type":"page"},{"content":"Here you can find some of the projects I\u0026rsquo;ve developed, showcasing my skills in machine learning and data science. Click on any project to learn more.\n","date":"1 January 2025","externalUrl":null,"permalink":"/projects/","section":"My Projects","summary":"A collection of machine learning projects I\u0026rsquo;ve worked on.","title":"My Projects","type":"projects"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"}]