
[{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/inference-optimization/","section":"Tags","summary":"","title":"Inference Optimization","type":"tags"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/categories/machine-learning/","section":"Categories","summary":"","title":"Machine Learning","type":"categories"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/categories/nlp/","section":"Categories","summary":"","title":"NLP","type":"categories"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/speculative-decoding/","section":"Tags","summary":"","title":"Speculative Decoding","type":"tags"},{"content":" TL;DR # Speculative decoding uses a small draft model or heuristic to propose a continuation of generated tokens, and the LLM scans the suggestions in parallel to verify the suggestions, or quickly correct them. The introduced parallelism makes it much faster than token-by-token generation, and unlike other optimizations, it does not sacrifice quality.\nThe Problem # Large language models are auto-regressive - they generate the next token by taking the past tokens as input at each step one at a time, which is inherently slow. It\u0026rsquo;s not primarily because the mathematical computations (the FLOPs) for predicting a single token are overwhelming. Instead, the critical bottleneck is memory bandwidth.\nModern LLMs can occupy hundreds of gigabytes or even terabytes of memory. These parameters typically reside in High-Bandwidth Memory (HBM) associated with the accelerator (like a GPU or TPU). However, to perform the computations needed to predict the next token, these parameters must be loaded into the much smaller, faster on-chip memory (cache or SRAM) of the compute units.\nIn autoregressive decoding, this massive data transfer - loading potentially terabytes of weights from HBM to cache - happens for every single token generated. This memory movement completely dominates the time taken for each step. The powerful compute cores of the accelerator end up spending most of their time waiting for data to arrive, leading to severe underutilization and low arithmetic intensity (the ratio of computational operations to memory access).\nThe LLM decoding operation is memory-bound as described above, and hence making compute units faster will not make the inference faster proportionally. Any truly effective acceleration strategy for the decoding phase must find a way to reduce the number of these costly memory-transfer cycles required per generated token. Hence, we would need to devise a way to parallelize an inherently sequential process.\nThe Solution # Speculative Decoding (SD) does exactly that - it speeds up the sequential decoding by generating multiple tokens for roughly the cost of one target model memory load cycle, and as a result, it significantly improves the efficiency of the decoding process.\nIt uses a draft then verify approach - drafting produces multiple next tokens using a faster/smaller model, and verifies them in parallel in the target/large LM.\nThe Target Model (Mq​): This is the large, powerful, and accurate LLM whose output distribution we want to replicate exactly, but is very slow.\nThe Draft Model (Mp​): This is a significantly smaller, and therefore much faster, language model or n-gram model. It might be a smaller version from the same model family, a distilled version, or even a different architecture altogether. Its role is not to be perfectly accurate, but to quickly generate plausible candidate tokens (the \u0026ldquo;draft\u0026rdquo;).\nThe core idea relies on two key observations:\nMany tokens are \u0026ldquo;easy\u0026rdquo;: Not all tokens in a generated response need the full power of a huge LLM - common phrases, grammatical structures, or repetitive sequences can be correctly generated by a much smaller draft model with high confidence. The large model must only be used for more nuanced generation tasks such as reasoning or factual recall - the \u0026ldquo;hard\u0026rdquo; tokens.\nParallel Verification is Cheap: While generating one token with the large target model Mq​ is slow due to the memory bandwidth bottleneck, verifying multiple candidate tokens (say, K draft tokens plus the original context) in a single, parallel forward pass through Mq​ takes roughly the same amount of time. This is because the dominant cost – loading the model weights – happens only once for the entire verification batch.\nSpeculative decoding leverages these observations. If the draft model\u0026rsquo;s predictions align well with what the target model would have generated, multiple tokens can be accepted in a single step, effectively amortizing the cost of the expensive target model inference over several tokens.\nThe drafting part is straightforward - you just generate K tokens from the draft model auto-regressively. The verification part, intuitively works for transformers because it bypasses the causal dependency in auto-regressive decoding by computing the logits of a sequence at once. Let\u0026rsquo;s look at some familiar code to clarify -\nAuto-Regressive Decoding (Single Token) # from transformers import AutoModelForCausalLM, AutoTokenizer model_name = \u0026#34;gpt2\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name) context = \u0026#34;The cat sat\u0026#34; input_ids = tokenizer(context, return_tensors=\u0026#34;pt\u0026#34;).input_ids # Step 1: Predict the next token only outputs = model(input_ids) next_token_logits = outputs.logits[:, -1, :] # Logits for the next token Speculative Decoding (Draft + Verify) # # Step 1 - get tokens from draft model drafted_tokens = [\u0026#34;on\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;mat\u0026#34;] drafted_sequence = context + \u0026#34; \u0026#34; + \u0026#34; \u0026#34;.join(drafted_tokens) # Tokenize the full sequence full_input_ids = tokenizer(drafted_sequence, return_tensors=\u0026#34;pt\u0026#34;).input_ids # One forward pass through the verifier model outputs = model(full_input_ids) logits = outputs.logits # Shape: [batch_size, seq_len, vocab_size] # To get the logits for each drafted token: draft_start = input_ids.shape[1] # First drafted token position for i, token in enumerate(drafted_tokens): position = draft_start + i - 1 # logits at position predict next token token_id = tokenizer.convert_tokens_to_ids(token) # Log probability the verifier would assign to the draft token prob = logits[0, position, token_id].softmax(dim=-1).item() print(f\u0026#34;Verifier probability for \u0026#39;{token}\u0026#39;: {prob}\u0026#34;) As you can see, we can directly get the assigned probabilities for each token at once, and hence compute the overall probability of the sequence in a single pass.\nThe last step is the acceptance/rejection token-by-token verification that decides which tokens to keep or reject from the draft - we simply follow the above idea, where we have the verifier probability of each drafted token, and check if it is the highest top token according to the verifier logits. This is the simplest acceptance/rejection method, called top-1 verification. In practice, we would use a generalized speculative decoding algorithm with Metropolis-Hastings-style acceptance criteria (often called \u0026ldquo;adaptive speculative decoding\u0026rdquo; or \u0026ldquo;accept/reject sampling\u0026rdquo;).\n# Start comparing drafted tokens one by one accepted_tokens = [] draft_start = input_ids.shape[1] # First position where drafts start for i, token in enumerate(draft_tokens): # Position in logits where the verifier predicts the NEXT token position = draft_start + i - 1 # position -1 because logits predict the next token # Verifier\u0026#39;s predicted token (top-1) verifier_top_token_id = logits[0, position, :].argmax().item() verifier_top_token = tokenizer.convert_ids_to_tokens([verifier_top_token_id])[0] # Drafted token ID drafted_token_id = tokenizer.convert_tokens_to_ids(token) # Compare verifier\u0026#39;s top token to drafted token if verifier_top_token_id == drafted_token_id: accepted_tokens.append(token) print(f\u0026#34;ACCEPTED token \u0026#39;{token}\u0026#39; at position {i+1}\u0026#34;) else: print(f\u0026#34;REJECTED token \u0026#39;{token}\u0026#39; at position {i+1}\u0026#34;) break # Stop at first mismatch Rejection Sampling Approach # In this, instead of just checking \u0026ldquo;is the token the same as verifier\u0026rsquo;s top prediction?\u0026rdquo; you give some probability of accepting even if the draft and verifier disagree - depending on how much they disagree (probabilistic acceptance).\nMetropolis-Hastings Acceptance Criteria # Pick a random number \u0026ldquo;r\u0026rdquo; from uniform distribution [0,1] For each token, get draft logit/probability \u0026ldquo;p_t\u0026rdquo; and the verifier logit/probability \u0026ldquo;q_t\u0026rdquo; Accept draft token if r \u0026lt; min(1, p_t/q_t) - essentially if the draft probability is much lower than the verifier probability, the chances of acceptance drop. This is where we see how much the models agree with each other. The following code illustrates the approach, along with how we then correct the remaining draft tokens if a token is rejected before reaching the end of current draft tokens.\nThe correction happens by sampling from the residual distribution max(0, (q(x) - p(x))). Intuitively, we want to do this to ensure that we ignore all the remaining token choices where the draft model logits \u0026gt; verifier logits.\nFor example, if -\nDraft model p(x) says:\n\u0026ldquo;cat\u0026rdquo;: 0.4 \u0026ldquo;dog\u0026rdquo;: 0.3 \u0026ldquo;mouse\u0026rdquo;: 0.3 Verifier q(x) says:\n\u0026ldquo;cat\u0026rdquo;: 0.2 \u0026ldquo;dog\u0026rdquo;: 0.5 \u0026ldquo;mouse\u0026rdquo;: 0.3 Draft model picked \u0026ldquo;cat\u0026rdquo;, but it got rejected. Now, we calculate residuals: residual(cat) = max(0, 0.2 - 0.4) = 0 residual(dog) = max(0, 0.5 - 0.3) = 0.2 residual(mouse) = max(0, 0.3 - 0.3) = 0\nIn this case, \u0026ldquo;dog\u0026rdquo; remains the only option. If you just pick from q(x) again, you might resample a token the draft model already explored and proposed, wasting compute and possibly biasing the results. There is an edge case, where the residual might become all \u0026lt; 0, in which case the algorithm must fallback to sampling from q(x) - I have no remaining unaccounted-for probability, so I\u0026rsquo;ll just sample from the verifier\u0026rsquo;s full distribution as if no draft proposal happened.\nimport random import torch accepted_tokens = [] draft_start = input_ids.shape[1] for i, token in enumerate(draft_tokens): position = draft_start + i - 1 # Get probabilities (softmax over logits) verifier_probs = logits[0, position, :].softmax(dim=-1) draft_token_id = tokenizer.convert_tokens_to_ids(token) q_t = verifier_probs[draft_token_id].item() # verifier prob p_t = draft_probs[i] # you must get this from the draft model when proposing tokens # Metropolis-Hastings acceptance rule r = random.uniform(0, 1) acceptance_threshold = min(1, p_t / q_t) if r \u0026lt; acceptance_threshold: accepted_tokens.append(token) print(f\u0026#34;ACCEPTED token \u0026#39;{token}\u0026#39; with r={r:.4f}, threshold={acceptance_threshold:.4f}\u0026#34;) else: print(f\u0026#34;REJECTED token \u0026#39;{token}\u0026#39; with r={r:.4f}, threshold={acceptance_threshold:.4f}\u0026#34;) break # Stop at first rejection # If rejected, resample from residual (q - p)+ if len(accepted_tokens) \u0026lt; len(draft_tokens): position = draft_start + len(accepted_tokens) - 1 residual = (verifier_probs - draft_probs_tensor).clamp(min=0) residual /= residual.sum() # Normalize next_token_id = torch.multinomial(residual, 1).item() next_token = tokenizer.convert_ids_to_tokens([next_token_id])[0] accepted_tokens.append(next_token) print(\u0026#34;Final accepted sequence:\u0026#34;, accepted_tokens) Bonus Token # If all the draft model tokens are accepted, indicating that the draft and target model are aligned, we can actually sample an additional token from the target model since we are already generating K logits from the model, giving us K+1 tokens - this gives us a bonus token for free! Further maximizing efficiency.\nKV Cache Efficiency # A critical aspect for efficiency, especially during the parallel verification step, is managing the Key-Value (KV) cache. Standard autoregressive decoding maintains a cache of activations from previous steps to avoid recomputing them. In speculative decoding, the target model processes multiple potential future states simultaneously. A naive implementation might require replicating the KV cache for each potential path, leading to excessive memory usage.\nOptimized inference engines address this challenge. Techniques like PagedAttention allow sharing parts of the KV cache between different sequences in the verification batch. This works similarly to virtual memory paging in operating systems, dividing the cache into blocks that can be shared and managed efficiently, preventing memory bloat and maintaining throughput even at larger batch sizes.\nChoosing Draft and Target Models # Usually draft models can be:\nA smaller model in the same LLM family (Gemma 2B -\u0026gt; Gemma 9B). Additionally, the smaller models can be further aligned through knowledge distillation.\nN-grams: Proposals are drawn by matching up to a chosen size n-grams from the prompt itself, effectively reusing previously seen text patterns.\nCustom MLP‐based speculator networks condition on both the context vectors and sampled tokens to predict future tokens, enabling learned proposal distributions beyond simple draft‐model outputs.\nEAGLE (Extrapolation Algorithm for Greater Language‐model Efficiency) draft models to extrapolate continuations, combining algorithmic lookahead with draft‐model verification for efficient, lossless sampling.\nKey Hyperparameters # K (number of draft tokens): Number of tokens proposed by the draft model per iteration.\nImportant because it affects the balance of more accepted tokens vs. higher draft cost \u0026amp; rejection probability. Optimal often 3-5. Depends on draft speed/accuracy. Draft Model Size/Arch: Choice of the smaller model (parameters, architecture).\nDirectly impacts draft latency (critical) and baseline acceptance rate. Aim for low latency. 10-20x smaller than target or use specialized architectures. Draft Model Alignment: How well the draft model predicts the target model\u0026rsquo;s distribution.\nPrimary driver of acceptance rate. Higher alignment = higher acceptance = more speedup. Use models from same family, or apply Knowledge distillation. Temperature: Controls randomness during sampling.\nHigher temp -\u0026gt; lower predictability -\u0026gt; lower acceptance rate -\u0026gt; lower speedup. Performance often peaks at low/mid temps. Align KD temp with inference temp. References # Leviathan, Y., Kalman, M., \u0026amp; Matias, Y. (2023). Fast Inference from Transformers via Speculative Decoding. ICML. Chen, X., et al. (2023). Accelerating Large Language Model Decoding with Speculative Sampling. arXiv preprint. Spector, B., \u0026amp; Murray, K. (2023). Accelerating LLM Inference with Staged Speculative Decoding. NeurIPS. Visuals \u0026amp; Diagrams # ","date":"12 May 2025","externalUrl":null,"permalink":"/posts/speculative-decoding/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eTL;DR \n    \u003cdiv id=\"tldr\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#tldr\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eSpeculative decoding uses a small draft model or heuristic to propose a continuation of generated tokens, and the LLM scans the suggestions in parallel to verify the suggestions, or quickly correct them. The introduced parallelism makes it much faster than token-by-token generation, and unlike other optimizations, it does not sacrifice quality.\u003c/p\u003e","title":"Speculative Decoding: 2x to 4x speedup of LLMs without quality loss","type":"posts"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/","section":"synaptic_radio","summary":"","title":"synaptic_radio","type":"page"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/transformers/","section":"Tags","summary":"","title":"Transformers","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/cnn/","section":"Tags","summary":"","title":"CNN","type":"tags"},{"content":" Project Goal # The objective was to implement a CNN from scratch\u0026hellip;\nDataset # Used the popular CIFAR-10 dataset containing 60,000 32x32 color images in 10 classes.\nModel Architecture # Designed a standard CNN with convolutional layers, ReLU activations, max-pooling, and fully connected layers.\n# Simplified PyTorch model definition import torch.nn as nn import torch.nn.functional as F class SimpleCNN(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) # ... more layers self.fc3 = nn.Linear(84, 10) def forward(self, x): # ... forward pass logic return x ","date":"20 March 2025","externalUrl":null,"permalink":"/projects/image-classification-cnn/","section":"My Projects","summary":"Built and trained a Convolutional Neural Network using PyTorch to classify images from the CIFAR-10 dataset.","title":"CNN for Image Classification (CIFAR-10)","type":"projects"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/portfolio/","section":"Tags","summary":"","title":"Portfolio","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/categories/projects/","section":"Categories","summary":"","title":"Projects","type":"categories"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"PyTorch","type":"tags"},{"content":"Hello! I\u0026rsquo;m [Your Name], a [Your Role] passionate about [Your Interests within ML/DS].\nThis blog serves as my digital notebook and portfolio where I share project write-ups, tutorials, and thoughts on machine learning concepts.\nYou can find my resume here (Place resume.pdf in the static/ folder).\nConnect with me via the links in the profile section or footer.\n","date":"1 January 2025","externalUrl":null,"permalink":"/about/","section":"synaptic_radio","summary":"\u003cp\u003eHello! I\u0026rsquo;m [Your Name], a [Your Role] passionate about [Your Interests within ML/DS].\u003c/p\u003e\n\u003cp\u003eThis blog serves as my digital notebook and portfolio where I share project write-ups, tutorials, and thoughts on machine learning concepts.\u003c/p\u003e","title":"About Me","type":"page"},{"content":"Here you can find some of the projects I\u0026rsquo;ve developed, showcasing my skills in machine learning and data science. Click on any project to learn more.\n","date":"1 January 2025","externalUrl":null,"permalink":"/projects/","section":"My Projects","summary":"A collection of machine learning projects I\u0026rsquo;ve worked on.","title":"My Projects","type":"projects"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]