
[{"content":" This series is designed to forge a deep, intuitive understanding of the mathematics that power the code I write every day as an ML engineer. Just as a traveler in Montreal might scrape by on a handful of French phrases but needs true fluency to think and live there, we often learn machine-learning theory and then struggle to translate it into code. Likewise, many engineers can train models and assemble practical toolkits without ever unpacking the mechanics beneath the surface. Here, we\u0026rsquo;ll sidestep heavy libraries like NumPy‚Äîat least at first‚Äîto examine each calculation by hand and demystify exactly how the math drives every line of code.\nChapter 1: Dot Product # Let\u0026rsquo;s start by implementing the dot product, the workhorse computation of machine learning.\nCalculating the Dot Product # Given two vectors, say \\(\\vec{a} = [a_1, a_2, \\dots, a_n]\\) and \\(\\vec{b} = [b_1, b_2, \\dots, b_n]\\), their dot product is found by multiplying corresponding elements and summing the results.\nimport math # Dot product of two vectors # [a1, a2, a3], [b1, b2, b3] -\u0026gt; a1*b1 + a2*b2 + a3*b3 def dot_product(v1, v2): # Ensure vectors are of the same length if len(v1) != len(v2): raise ValueError(\u0026#34;Vectors must be of the same length\u0026#34;) return sum(x * y for x, y in zip(v1, v2)) # all hail zip!! # Example vectors a = [1, 3, -5] b = [4, -2, -1] # Calculate and print the dot product ab_dot_product = dot_product(a, b) print(f\u0026#34;Vector a: {a}\u0026#34;) print(f\u0026#34;Vector b: {b}\u0026#34;) print(f\u0026#34;Dot product (a ¬∑ b): {ab_dot_product}\u0026#34;) This will output:\nVector a: [1, 3, -5] Vector b: [4, -2, -1] Dot product (a ¬∑ b): 3 What\u0026rsquo;s happening here?\nThe formula we\u0026rsquo;ve just implemented is:\n$$ \\vec{a} \\cdot \\vec{b} = a_1 b_1 + a_2 b_2 + \\dots + a_n b_n $$For our example vectors \\(\\vec{a} = [1, 3, -5]\\) and \\(\\vec{b} = [4, -2, -1]\\):\n$$\\vec{a} \\cdot \\vec{b} = (1)(4) + (3)(-2) + (-5)(-1) = 4 - 6 + 5 = 3 $$The result, 3, is a single number (a scalar), which is why the dot product is often called the scalar product.\nThe Geometric Intuition: Angles and Magnitudes # The real magic of the dot product comes from its geometric interpretation:\n$$ \\vec{a} \\cdot \\vec{b} = |\\vec{a}| |\\vec{b}| \\cos(\\theta) $$Where:\n\\(|\\vec{a}|\\) is the magnitude (or length) of vector \\(\\vec{a}\\). \\(|\\vec{b}|\\) is the magnitude of vector \\(\\vec{b}\\). \\(\\theta\\) is the angle between vectors \\(\\vec{a}\\) and \\(\\vec{b}\\). Let\u0026rsquo;s add functions to calculate the magnitude of a vector and the angle between two vectors.\n# Magnitude of a vector def magnitude(v): return math.sqrt(sum(x**2 for x in v)) # Angle between vectors in degrees def angle_between(v1, v2): dp = dot_product(v1, v2) mag1 = magnitude(v1) mag2 = magnitude(v2) # Prevent division by zero if one of the vectors is a zero vector if mag1 == 0 or mag2 == 0: return 0 # Or raise an error, or handle as appropriate # Clamp the cos_theta value to the range [-1, 1] to avoid math domain errors with acos cos_theta_val = dp / (mag1 * mag2) cos_theta = max(-1.0, min(1.0, cos_theta_val)) theta_rad = math.acos(cos_theta) return round(math.degrees(theta_rad), 2) # Calculate magnitudes mag_a = magnitude(a) mag_b = magnitude(b) # Calculate angle angle_ab = angle_between(a, b) print(f\u0026#34;Magnitude of a (||a||): {round(mag_a, 2)}\u0026#34;) print(f\u0026#34;Magnitude of b (||b||): {round(mag_b, 2)}\u0026#34;) print(f\u0026#34;Angle between a and b (Œ∏): {angle_ab} degrees\u0026#34;) This will output:\nMagnitude of a (||a||): 5.92 Magnitude of b (||b||): 4.58 Angle between a and b (Œ∏): 83.61 degrees Interpreting the Dot Product\u0026rsquo;s Sign:\nThe formula \\(\\vec{a} \\cdot \\vec{b} = |\\vec{a}| |\\vec{b}| \\cos(\\theta)\\) tells us a lot about the vectors\u0026rsquo; orientation:\nIf \\(\\vec{a} \\cdot \\vec{b} \u0026gt; 0\\): \\(\\cos(\\theta) \u0026gt; 0\\), meaning \\(0^\\circ \\leq \\theta \u0026lt; 90^\\circ\\). The vectors point in the same general direction. If \\(\\vec{a} \\cdot \\vec{b} = 0\\): \\(\\cos(\\theta) = 0\\), meaning \\(\\theta = 90^\\circ\\). The vectors are perpendicular (orthogonal). This is a crucial concept in many areas of math and computer science. If \\(\\vec{a} \\cdot \\vec{b} \u0026lt; 0\\): \\(\\cos(\\theta) \u0026lt; 0\\), meaning \\(90^\\circ \u0026lt; \\theta \\leq 180^\\circ\\). The vectors point in opposite general directions. Our result of 3 (positive) and an angle of 83.61 degrees aligns with this: the vectors are generally pointing in a similar direction.\nApplications in Machine Learning and Beyond # The dot product isn\u0026rsquo;t just an abstract mathematical operation; it\u0026rsquo;s incredibly useful:\nCosine Similarity: The term \\(\\cos(\\theta)\\) can be isolated: \\(\\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}| |\\vec{b}|}\\). This is the cosine similarity, a metric ranging from -1 to 1 that measures how similar the direction of two vectors is. Search Engines \u0026amp; Recommendation Systems: Used to compare query vectors with document vectors or user preference vectors with item vectors. High cosine similarity means high relevance. Natural Language Processing (NLP): Word embeddings (like Word2Vec, GloVe, or those from Transformers) represent words as vectors. Cosine similarity between these vectors can indicate semantic similarity between words. Large Language Models (LLMs): Attention mechanisms, a core component of Transformers (which power LLMs like GPT), heavily rely on dot products to determine how much \u0026ldquo;attention\u0026rdquo; one part of a sequence should pay to another. Essentially, it\u0026rsquo;s calculating similarities between query, key, and value vectors. Projections: The dot product helps in projecting one vector onto another. This tells us how much of one vector lies in the direction of another. Imagine shining a light perpendicularly onto vector \\(\\vec{b}\\); the length of the shadow of \\(\\vec{a}\\) on \\(\\vec{b}\\) is related to the dot product.\nGeometric Transformations: As we\u0026rsquo;ll see with matrices, dot products are fundamental to rotating, scaling, and shearing vectors in 2D/3D graphics.\nPhysics - Work Done: A classic example. If a force \\(\\vec{F}\\) acts on an object causing a displacement \\(\\vec{d}\\), the work done (\\(W\\)) is:\n$$ W = \\vec{F} \\cdot \\vec{d} $$Only the component of the force that is in the direction of movement contributes to the work. If the force is perpendicular to the displacement, the dot product is zero, and no work is done by that force in that direction. (If your attention vector while reading this is perpendicular to the content, your learning \u0026ldquo;work done\u0026rdquo; might be zero! üìêüòâ)\nExtending to Matrix-Vector Products # Now, let\u0026rsquo;s see how the dot product extends to multiplying a matrix by a vector.\nConsider a matrix \\(A\\) and a vector \\(\\vec{x}\\):\nA = [ [a‚ÇÅ‚ÇÅ, a‚ÇÅ‚ÇÇ, ..., a‚ÇÅ‚Çô], // row‚ÇÅ [a‚ÇÇ‚ÇÅ, a‚ÇÇ‚ÇÇ, ..., a‚ÇÇ‚Çô], // row‚ÇÇ ... [a‚Çò‚ÇÅ, a‚Çò‚ÇÇ, ..., a‚Çò‚Çô] // row‚Çò ] x = [x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]·µÄ // Transpose for column vector notation The product \\(A \\cdot \\vec{x}\\) is a new vector \\(\\vec{y}\\), where each element of \\(\\vec{y}\\) is the dot product of a row from \\(A\\) with the vector \\(\\vec{x}\\).\n# Matrix-vector dot product def matrix_vector_product(matrix, vector): # Ensure the number of columns in the matrix matches the length of the vector if not matrix or (len(matrix[0]) != len(vector)): raise ValueError(\u0026#34;Number of columns in matrix must match length of vector\u0026#34;) return [dot_product(row, vector) for row in matrix] # Example Matrix and Vector A = [ [2, 1, 0], [-1, 3, 2], [0, 0, 1] ] x_vec = [4, -2, -1] # Using our previous vector b as x for this example # Calculate and print the matrix-vector product y_vec = matrix_vector_product(A, x_vec) print(f\u0026#34;\\nMatrix A:\\n{A[0]}\\n{A[1]}\\n{A[2]}\u0026#34;) print(f\u0026#34;Vector x: {x_vec}\u0026#34;) print(f\u0026#34;Matrix-vector product (A ¬∑ x): {y_vec}\u0026#34;) This will output:\nMatrix A: [2, 1, 0] [-1, 3, 2] [0, 0, 1] Vector x: [4, -2, -1] Matrix-vector product (A ¬∑ x): [6, -12, -1] What does this mean?\nThe resulting vector \\(\\vec{y}\\) is:\n$$ \\vec{y} = \\begin{bmatrix} \\text{row}_1 \\cdot \\vec{x} \\ \\text{row}_2 \\cdot \\vec{x} \\ \\vdots \\ \\text{row}_m \\cdot \\vec{x} \\end{bmatrix} $$For our example: \\(y_1 = \\text{row}_1 \\cdot \\vec{x} = [2, 1, 0] \\cdot [4, -2, -1] = (2)(4) + (1)(-2) + (0)(-1) = 8 - 2 + 0 = 6\\) \\(y_2 = \\text{row}_2 \\cdot \\vec{x} = [-1, 3, 2] \\cdot [4, -2, -1] = (-1)(4) + (3)(-2) + (2)(-1) = -4 - 6 - 2 = -12\\) \\(y_3 = \\text{row}_3 \\cdot \\vec{x} = [0, 0, 1] \\cdot [4, -2, -1] = (0)(4) + (0)(-2) + (1)(-1) = 0 + 0 - 1 = -1\\)\nSo, \\(\\vec{y} = [6, -12, -1]\\).\nApplications of Matrix-Vector Products:\nWeighted Sums in Neural Networks: This is exactly how inputs are combined in a neuron. If \\(\\vec{x}\\) is a vector of inputs and a row of matrix \\(A\\) contains the weights for those inputs, their dot product is the weighted sum, which then goes into an activation function. The entire matrix \\(A\\) can represent the weights of a layer. Systems of Linear Equations: A system of linear equations can be compactly written as \\(A\\vec{x} = \\vec{b}\\), where we solve for \\(\\vec{x}\\). Linear Transformations: In geometry, multiplying a vector by a matrix can represent a linear transformation like rotation, scaling, or shearing of that vector. Each row of the matrix contributes to transforming the input vector into the output vector. The resulting vector \\(\\vec{y}\\) tells you where the vector \\(\\vec{x}\\) lands after being transformed by \\(A\\). Phew! That\u0026rsquo;s a lot of work to understand something basic, but upon this stone we shall build our non-religious gathering place to preach the gospel of ML!\n","date":"13 May 2025","externalUrl":null,"permalink":"/posts/intuitive-linear-algebra-with-python/","section":"Posts","summary":"\u003cp\u003eThis series is designed to forge a deep, intuitive understanding of the mathematics that power the code I write every day as an ML engineer. Just as a traveler in Montreal might scrape by on a handful of French phrases but needs true fluency to think and live there, we often learn machine-learning theory and then struggle to translate it into code. Likewise, many engineers can train models and assemble practical toolkits without ever unpacking the mechanics beneath the surface. Here, we\u0026rsquo;ll sidestep heavy libraries like NumPy‚Äîat least at first‚Äîto examine each calculation by hand and demystify exactly how the math drives every line of code.\u003c/p\u003e","title":"üêç Python Guide to Linear Algebra for Machine Learning for Engineers","type":"posts"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/tags/code-first/","section":"Tags","summary":"","title":"Code-First","type":"tags"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/categories/linear-algebra/","section":"Categories","summary":"","title":"Linear Algebra","type":"categories"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/tags/linear-algebra/","section":"Tags","summary":"","title":"Linear Algebra","type":"tags"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/categories/machine-learning/","section":"Categories","summary":"","title":"Machine Learning","type":"categories"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/categories/math/","section":"Categories","summary":"","title":"Math","type":"categories"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/tags/math/","section":"Tags","summary":"","title":"Math","type":"tags"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/categories/python/","section":"Categories","summary":"","title":"Python","type":"categories"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/","section":"synaptic_radio","summary":"","title":"synaptic_radio","type":"page"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"13 May 2025","externalUrl":null,"permalink":"/tags/vectors/","section":"Tags","summary":"","title":"Vectors","type":"tags"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/inference-optimization/","section":"Tags","summary":"","title":"Inference Optimization","type":"tags"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/categories/nlp/","section":"Categories","summary":"","title":"NLP","type":"categories"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/speculative-decoding/","section":"Tags","summary":"","title":"Speculative Decoding","type":"tags"},{"content":" TL;DR # Speculative decoding uses a small draft model or heuristic to propose a continuation of generated tokens, and the LLM scans the suggestions in parallel to verify the suggestions, or quickly correct them. The introduced parallelism makes it much faster than token-by-token generation, and unlike other optimizations, it does not sacrifice quality.\nThe Problem # Large language models are auto-regressive - they generate the next token by taking the past tokens as input at each step one at a time, which is inherently slow. It\u0026rsquo;s not primarily because the mathematical computations (the FLOPs) for predicting a single token are overwhelming. Instead, the critical bottleneck is memory bandwidth.\nModern LLMs can occupy hundreds of gigabytes or even terabytes of memory. These parameters typically reside in High-Bandwidth Memory (HBM) associated with the accelerator (like a GPU or TPU). However, to perform the computations needed to predict the next token, these parameters must be loaded into the much smaller, faster on-chip memory (cache or SRAM) of the compute units.\nIn autoregressive decoding, this massive data transfer - loading potentially terabytes of weights from HBM to cache - happens for every single token generated. This memory movement completely dominates the time taken for each step. The powerful compute cores of the accelerator end up spending most of their time waiting for data to arrive, leading to:\nSevere underutilization of compute resources Low arithmetic intensity (the ratio of computational operations to memory access) The LLM decoding operation is memory-bound as described above, and hence making compute units faster will not make the inference faster proportionally. Any truly effective acceleration strategy for the decoding phase must find a way to reduce the number of these costly memory-transfer cycles required per generated token. Hence, we would need to devise a way to parallelize an inherently sequential process.\nThe Solution # Speculative Decoding (SD) does exactly that - it speeds up the sequential decoding by generating multiple tokens for roughly the cost of one target model memory load cycle, and as a result, it significantly improves the efficiency of the decoding process.\nIt uses a draft then verify approach:\nDrafting produces multiple next tokens using a faster/smaller model Verification checks these tokens in parallel in the target/large LM The key players:\nThe Target Model (Mq‚Äã): This is the large, powerful, and accurate LLM whose output distribution we want to replicate exactly, but is very slow. The Draft Model (Mp‚Äã): This is a significantly smaller, and therefore much faster, language model or n-gram model. It might be a smaller version from the same model family, a distilled version, or even a different architecture altogether. Its role is not to be perfectly accurate, but to quickly generate plausible candidate tokens (the \u0026ldquo;draft\u0026rdquo;). The core idea relies on two key observations:\nMany tokens are \u0026ldquo;easy\u0026rdquo;: Not all tokens in a generated response need the full power of a huge LLM - common phrases, grammatical structures, or repetitive sequences can be correctly generated by a much smaller draft model with high confidence. The large model must only be used for more nuanced generation tasks such as reasoning or factual recall - the \u0026ldquo;hard\u0026rdquo; tokens.\nParallel Verification is Cheap: While generating one token with the large target model Mq‚Äã is slow due to the memory bandwidth bottleneck, verifying multiple candidate tokens (say, K draft tokens plus the original context) in a single, parallel forward pass through Mq‚Äã takes roughly the same amount of time. This is because the dominant cost ‚Äì loading the model weights ‚Äì happens only once for the entire verification batch.\nSpeculative decoding leverages these observations. If the draft model\u0026rsquo;s predictions align well with what the target model would have generated, multiple tokens can be accepted in a single step, effectively amortizing the cost of the expensive target model inference over several tokens.\nThe drafting part is straightforward - you just generate K tokens from the draft model auto-regressively. The verification part, intuitively works for transformers because it bypasses the causal dependency in auto-regressive decoding by computing the logits of a sequence at once. Let\u0026rsquo;s look at some familiar code to clarify -\nAuto-Regressive Decoding (Single Token) # from transformers import AutoModelForCausalLM, AutoTokenizer # Setup model and tokenizer model_name = \u0026#34;gpt2\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name) # Prepare input context context = \u0026#34;The cat sat\u0026#34; input_ids = tokenizer(context, return_tensors=\u0026#34;pt\u0026#34;).input_ids # Generate single token prediction outputs = model(input_ids) next_token_logits = outputs.logits[:, -1, :] # Logits for the next token Speculative Decoding (Draft + Verify) # # Step 1: Draft Model - Generate candidate tokens drafted_tokens = [\u0026#34;on\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;mat\u0026#34;] drafted_sequence = context + \u0026#34; \u0026#34; + \u0026#34; \u0026#34;.join(drafted_tokens) # Step 2: Prepare full sequence for verification full_input_ids = tokenizer(drafted_sequence, return_tensors=\u0026#34;pt\u0026#34;).input_ids # Step 3: Verify with target model (single forward pass) outputs = model(full_input_ids) logits = outputs.logits # Shape: [batch_size, seq_len, vocab_size] # Step 4: Extract verification probabilities for each drafted token draft_start = input_ids.shape[1] # First drafted token position for i, token in enumerate(drafted_tokens): position = draft_start + i - 1 # logits at position predict next token token_id = tokenizer.convert_tokens_to_ids(token) # Log probability the verifier would assign to the draft token prob = logits[0, position, token_id].softmax(dim=-1).item() print(f\u0026#34;Verifier probability for \u0026#39;{token}\u0026#39;: {prob}\u0026#34;) As you can see, we can directly get the assigned probabilities for each token at once, and hence compute the overall probability of the sequence in a single pass.\nThe last step is the acceptance/rejection token-by-token verification that decides which tokens to keep or reject from the draft - we simply follow the above idea, where we have the verifier probability of each drafted token, and check if it is the highest top token according to the verifier logits. This is the simplest acceptance/rejection method, called top-1 verification. In practice, we would use a generalized speculative decoding algorithm with Metropolis-Hastings-style acceptance criteria (often called \u0026ldquo;adaptive speculative decoding\u0026rdquo; or \u0026ldquo;accept/reject sampling\u0026rdquo;).\n# Top-1 Verification Implementation # Setup for token verification accepted_tokens = [] draft_start = input_ids.shape[1] # First position where drafts start # Step 1: Iterate through each drafted token for verification for i, token in enumerate(draft_tokens): # Position in logits where the verifier predicts the NEXT token position = draft_start + i - 1 # position -1 because logits predict the next token # Step 2: Get verifier\u0026#39;s top prediction verifier_top_token_id = logits[0, position, :].argmax().item() verifier_top_token = tokenizer.convert_ids_to_tokens([verifier_top_token_id])[0] # Step 3: Get drafted token ID for comparison drafted_token_id = tokenizer.convert_tokens_to_ids(token) # Step 4: Accept/reject based on exact match with top prediction if verifier_top_token_id == drafted_token_id: accepted_tokens.append(token) print(f\u0026#34;ACCEPTED token \u0026#39;{token}\u0026#39; at position {i+1}\u0026#34;) else: print(f\u0026#34;REJECTED token \u0026#39;{token}\u0026#39; at position {i+1}\u0026#34;) break # Stop at first mismatch Rejection Sampling Approach # In this, instead of just checking \u0026ldquo;is the token the same as verifier\u0026rsquo;s top prediction?\u0026rdquo; you give some probability of accepting even if the draft and verifier disagree - depending on how much they disagree (probabilistic acceptance).\nMetropolis-Hastings Acceptance Criteria # Pick a random number \u0026ldquo;r\u0026rdquo; from uniform distribution [0,1] For each token, get draft logit/probability \u0026ldquo;p_t\u0026rdquo; and the verifier logit/probability \u0026ldquo;q_t\u0026rdquo; Accept draft token if r \u0026lt; min(1, p_t/q_t) - essentially if the draft probability is much lower than the verifier probability, the chances of acceptance drop. This is where we see how much the models agree with each other. The following code illustrates the approach, along with how we then correct the remaining draft tokens if a token is rejected before reaching the end of current draft tokens.\nThe correction happens by sampling from the residual distribution max(0, (q(x) - p(x))). Intuitively, we want to do this to ensure that we ignore all the remaining token choices where the draft model logits \u0026gt; verifier logits.\nFor example, if -\nDraft model p(x) says:\n\u0026ldquo;cat\u0026rdquo;: 0.4 \u0026ldquo;dog\u0026rdquo;: 0.3 \u0026ldquo;mouse\u0026rdquo;: 0.3 Verifier q(x) says:\n\u0026ldquo;cat\u0026rdquo;: 0.2 \u0026ldquo;dog\u0026rdquo;: 0.5 \u0026ldquo;mouse\u0026rdquo;: 0.3 Draft model picked \u0026ldquo;cat\u0026rdquo;, but it got rejected. Now, we calculate residuals:\nresidual(cat) = max(0, 0.2 - 0.4) = 0 residual(dog) = max(0, 0.5 - 0.3) = 0.2 residual(mouse) = max(0, 0.3 - 0.3) = 0 In this case, \u0026ldquo;dog\u0026rdquo; remains the only option. If you just pick from q(x) again, you might resample a token the draft model already explored and proposed, wasting compute and possibly biasing the results. There is an edge case, where the residual might become all \u0026lt; 0, in which case the algorithm must fallback to sampling from q(x) - I have no remaining unaccounted-for probability, so I\u0026rsquo;ll just sample from the verifier\u0026rsquo;s full distribution as if no draft proposal happened.\nimport random import torch # Setup for Metropolis-Hastings acceptance criteria accepted_tokens = [] draft_start = input_ids.shape[1] # Step 1: Process drafted tokens with probabilistic acceptance for i, token in enumerate(draft_tokens): position = draft_start + i - 1 # Step 2: Get probability distributions from both models verifier_probs = logits[0, position, :].softmax(dim=-1) draft_token_id = tokenizer.convert_tokens_to_ids(token) # Extract specific token probabilities q_t = verifier_probs[draft_token_id].item() # verifier prob p_t = draft_probs[i] # you must get this from the draft model when proposing tokens # Step 3: Apply Metropolis-Hastings acceptance rule r = random.uniform(0, 1) acceptance_threshold = min(1, p_t / q_t) if r \u0026lt; acceptance_threshold: accepted_tokens.append(token) print(f\u0026#34;ACCEPTED token \u0026#39;{token}\u0026#39; with r={r:.4f}, threshold={acceptance_threshold:.4f}\u0026#34;) else: print(f\u0026#34;REJECTED token \u0026#39;{token}\u0026#39; with r={r:.4f}, threshold={acceptance_threshold:.4f}\u0026#34;) break # Stop at first rejection # Step 4: Handle rejection with residual sampling if len(accepted_tokens) \u0026lt; len(draft_tokens): position = draft_start + len(accepted_tokens) - 1 # Calculate and normalize residual distribution residual = (verifier_probs - draft_probs_tensor).clamp(min=0) residual /= residual.sum() # Normalize # Sample from residual distribution next_token_id = torch.multinomial(residual, 1).item() next_token = tokenizer.convert_ids_to_tokens([next_token_id])[0] accepted_tokens.append(next_token) print(\u0026#34;Final accepted sequence:\u0026#34;, accepted_tokens) Bonus Token # If all the draft model tokens are accepted, indicating that the draft and target model are aligned, we can actually sample an additional token from the target model since we are already generating K logits from the model, giving us K+1 tokens - this gives us a bonus token for free! Further maximizing efficiency.\nKV Cache Efficiency # A critical aspect for efficiency, especially during the parallel verification step, is managing the Key-Value (KV) cache. Standard autoregressive decoding maintains a cache of activations from previous steps to avoid recomputing them. In speculative decoding, the target model processes multiple potential future states simultaneously. A naive implementation might require replicating the KV cache for each potential path, leading to excessive memory usage.\nOptimized inference engines address this challenge. Techniques like PagedAttention allow sharing parts of the KV cache between different sequences in the verification batch. This works similarly to virtual memory paging in operating systems, dividing the cache into blocks that can be shared and managed efficiently, preventing memory bloat and maintaining throughput even at larger batch sizes.\nChoosing Draft and Target Models # Usually draft models can be:\nA smaller model in the same LLM family (Gemma 2B -\u0026gt; Gemma 9B). Additionally, the smaller models can be further aligned through knowledge distillation.\nN-grams: Proposals are drawn by matching up to a chosen size n-grams from the prompt itself, effectively reusing previously seen text patterns.\nCustom MLP‚Äêbased speculator networks condition on both the context vectors and sampled tokens to predict future tokens, enabling learned proposal distributions beyond simple draft‚Äêmodel outputs.\nEAGLE (Extrapolation Algorithm for Greater Language‚Äêmodel Efficiency) draft models to extrapolate continuations, combining algorithmic lookahead with draft‚Äêmodel verification for efficient, lossless sampling.\nKey Hyperparameters # K (number of draft tokens):\nNumber of tokens proposed by the draft model per iteration Important because it affects the balance of more accepted tokens vs. higher draft cost \u0026amp; rejection probability Optimal often 3-5. Depends on draft speed/accuracy Draft Model Size/Arch:\nChoice of the smaller model (parameters, architecture) Directly impacts draft latency (critical) and baseline acceptance rate Aim for low latency. 10-20x smaller than target or use specialized architectures Draft Model Alignment:\nHow well the draft model predicts the target model\u0026rsquo;s distribution Primary driver of acceptance rate. Higher alignment = higher acceptance = more speedup Use models from same family, or apply Knowledge distillation Temperature:\nControls randomness during sampling Higher temp -\u0026gt; lower predictability -\u0026gt; lower acceptance rate -\u0026gt; lower speedup Performance often peaks at low/mid temps. Align KD temp with inference temp References # Leviathan, Y., Kalman, M., \u0026amp; Matias, Y. (2023). Fast Inference from Transformers via Speculative Decoding. ICML. Chen, X., et al. (2023). Accelerating Large Language Model Decoding with Speculative Sampling. arXiv preprint. Spector, B., \u0026amp; Murray, K. (2023). Accelerating LLM Inference with Staged Speculative Decoding. NeurIPS. ","date":"12 May 2025","externalUrl":null,"permalink":"/posts/speculative-decoding/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eTL;DR \n    \u003cdiv id=\"tldr\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#tldr\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eSpeculative decoding\u003c/strong\u003e uses a small draft model or heuristic to propose a continuation of generated tokens, and the LLM scans the suggestions in parallel to verify the suggestions, or quickly correct them. The introduced parallelism makes it much faster than token-by-token generation, and unlike other optimizations, it does not sacrifice quality.\u003c/p\u003e","title":"Speculative Decoding: 2x to 4x speedup of LLMs without quality loss","type":"posts"},{"content":"","date":"12 May 2025","externalUrl":null,"permalink":"/tags/transformers/","section":"Tags","summary":"","title":"Transformers","type":"tags"},{"content":"I\u0026rsquo;m Anshu, an ML Engineer with 8+ years of experience at various hands-on problems in machine learning. This blog is my attempt to share some learning and distill ideas, along side showcasing some work and thoughts on different topics. Currently working as an Lead R\u0026amp;D Engineer at BenchSci.\nConnect with me via the links in the profile section or footer.\n","date":"1 January 2025","externalUrl":null,"permalink":"/about/","section":"synaptic_radio","summary":"\u003cp\u003eI\u0026rsquo;m Anshu, an ML Engineer with 8+ years of experience at various hands-on problems in machine learning. This blog is my attempt to share some learning and distill ideas, along side showcasing some work and thoughts on different topics. Currently working as an Lead R\u0026amp;D Engineer at BenchSci.\u003c/p\u003e","title":"About Me üá®üá¶ üíª üåá","type":"page"},{"content":"Here you can find some of the projects I\u0026rsquo;ve developed, showcasing my skills in machine learning and data science. Click on any project to learn more.\n","date":"1 January 2025","externalUrl":null,"permalink":"/projects/","section":"My Projects","summary":"A collection of machine learning projects I\u0026rsquo;ve worked on.","title":"My Projects","type":"projects"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]