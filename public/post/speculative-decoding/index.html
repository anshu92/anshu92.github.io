<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="Understand how speculative decoding achieves 2-4x faster LLM inference without compromising output quality. This technique uses a smaller model to draft tokens that are verified in parallel by the main model, solving the memory bandwidth bottleneck.">
<title>Speculative Decoding: 2x to 4x speedup of LLMs without quality loss</title>

<link rel='canonical' href='http://localhost:1313/post/speculative-decoding/'>

<link rel="stylesheet" href="/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css"><meta property='og:title' content="Speculative Decoding: 2x to 4x speedup of LLMs without quality loss">
<meta property='og:description' content="Understand how speculative decoding achieves 2-4x faster LLM inference without compromising output quality. This technique uses a smaller model to draft tokens that are verified in parallel by the main model, solving the memory bandwidth bottleneck.">
<meta property='og:url' content='http://localhost:1313/post/speculative-decoding/'>
<meta property='og:site_name' content='Synaptic Radio'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='LLM' /><meta property='article:tag' content='Speculative Decoding' /><meta property='article:tag' content='Transformers' /><meta property='article:tag' content='Inference Optimization' /><meta property='article:published_time' content='2025-05-12T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2025-05-12T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="Speculative Decoding: 2x to 4x speedup of LLMs without quality loss">
<meta name="twitter:description" content="Understand how speculative decoding achieves 2-4x faster LLM inference without compromising output quality. This technique uses a smaller model to draft tokens that are verified in parallel by the main model, solving the memory bandwidth bottleneck.">
    <link rel="shortcut icon" href="/img/brain-logo.png" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu_f26b2bb4b5ef91e9.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">üéß</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Synaptic Radio</a></h1>
            <h2 class="site-description">I&#39;m Anshu, a machine learning engineer with nine years of experience turning first-principles ideas into prototypes and production systems. Here, I share ideas, practical insights and best practices to help fellow engineers accelerate their ML projects.</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://www.linkedin.com/in/anshu92/'
                        target="_blank"
                        title="LinkedIn"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="48px" height="48px"><path fill="#0078d4" d="M42,37c0,2.762-2.238,5-5,5H11c-2.761,0-5-2.238-5-5V11c0-2.762,2.239-5,5-5h26c2.762,0,5,2.238,5,5	V37z"/><path d="M30,37V26.901c0-1.689-0.819-2.698-2.192-2.698c-0.815,0-1.414,0.459-1.779,1.364	c-0.017,0.064-0.041,0.325-0.031,1.114L26,37h-7V18h7v1.061C27.022,18.356,28.275,18,29.738,18c4.547,0,7.261,3.093,7.261,8.274	L37,37H30z M11,37V18h3.457C12.454,18,11,16.528,11,14.499C11,12.472,12.478,11,14.514,11c2.012,0,3.445,1.431,3.486,3.479	C18,16.523,16.521,18,14.485,18H18v19H11z" opacity=".05"/><path d="M30.5,36.5v-9.599c0-1.973-1.031-3.198-2.692-3.198c-1.295,0-1.935,0.912-2.243,1.677	c-0.082,0.199-0.071,0.989-0.067,1.326L25.5,36.5h-6v-18h6v1.638c0.795-0.823,2.075-1.638,4.238-1.638	c4.233,0,6.761,2.906,6.761,7.774L36.5,36.5H30.5z M11.5,36.5v-18h6v18H11.5z M14.457,17.5c-1.713,0-2.957-1.262-2.957-3.001	c0-1.738,1.268-2.999,3.014-2.999c1.724,0,2.951,1.229,2.986,2.989c0,1.749-1.268,3.011-3.015,3.011H14.457z" opacity=".07"/><path fill="#fff" d="M12,19h5v17h-5V19z M14.485,17h-0.028C12.965,17,12,15.888,12,14.499C12,13.08,12.995,12,14.514,12	c1.521,0,2.458,1.08,2.486,2.499C17,15.887,16.035,17,14.485,17z M36,36h-5v-9.099c0-2.198-1.225-3.698-3.192-3.698	c-1.501,0-2.313,1.012-2.707,1.99C24.957,25.543,25,26.511,25,27v9h-5V19h5v2.616C25.721,20.5,26.85,19,29.738,19	c3.578,0,6.261,2.25,6.261,7.274L36,36L36,36z"/></svg> 
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/categories/' >
                
                
                
                <span>Categories</span>
            </a>
        </li>
        
        
        <li >
            <a href='/tags/' >
                
                
                
                <span>Tags</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>Dark Mode</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#tldr">TL;DR</a></li>
    <li><a href="#the-problem">The Problem</a></li>
    <li><a href="#the-solution">The Solution</a>
      <ol>
        <li><a href="#auto-regressive-decoding-single-token">Auto-Regressive Decoding (Single Token)</a></li>
        <li><a href="#speculative-decoding-draft--verify">Speculative Decoding (Draft + Verify)</a></li>
        <li><a href="#rejection-sampling-approach">Rejection Sampling Approach</a>
          <ol>
            <li><a href="#metropolis-hastings-acceptance-criteria">Metropolis-Hastings Acceptance Criteria</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#bonus-token">Bonus Token</a></li>
    <li><a href="#kv-cache-efficiency">KV Cache Efficiency</a></li>
    <li><a href="#choosing-draft-and-target-models">Choosing Draft and Target Models</a></li>
    <li><a href="#key-hyperparameters">Key Hyperparameters</a></li>
    <li><a href="#references">References</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/machine-learning/" >
                Machine Learning
            </a>
        
            <a href="/categories/nlp/" >
                NLP
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/post/speculative-decoding/">Speculative Decoding: 2x to 4x speedup of LLMs without quality loss</a>
        </h2>
    
        
        <h3 class="article-subtitle">
            Understand how speculative decoding achieves 2-4x faster LLM inference without compromising output quality. This technique uses a smaller model to draft tokens that are verified in parallel by the main model, solving the memory bandwidth bottleneck.
        </h3>
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">May 12, 2025</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    11 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="tldr">TL;DR
</h2><p><strong>Speculative decoding</strong> uses a small draft model or heuristic to propose a continuation of generated tokens, and the LLM scans the suggestions in parallel to verify the suggestions, or quickly correct them. The introduced parallelism makes it much faster than token-by-token generation, and unlike other optimizations, it does not sacrifice quality.</p>
<h2 id="the-problem">The Problem
</h2><p>Large language models are <strong>auto-regressive</strong> - they generate the next token by taking the past tokens as input at each step one at a time, which is inherently slow. It&rsquo;s not primarily because the mathematical computations (the FLOPs) for predicting a single token are overwhelming. Instead, the critical bottleneck is <strong>memory bandwidth</strong>.</p>
<p>Modern LLMs can occupy hundreds of gigabytes or even terabytes of memory. These parameters typically reside in High-Bandwidth Memory (HBM) associated with the accelerator (like a GPU or TPU). However, to perform the computations needed to predict the next token, these parameters must be loaded into the much smaller, faster on-chip memory (cache or SRAM) of the compute units.</p>
<p>In autoregressive decoding, this massive data transfer - loading potentially terabytes of weights from HBM to cache - happens for every single token generated. This memory movement completely dominates the time taken for each step. The powerful compute cores of the accelerator end up spending most of their time waiting for data to arrive, leading to:</p>
<ul>
<li><strong>Severe underutilization</strong> of compute resources</li>
<li><strong>Low arithmetic intensity</strong> (the ratio of computational operations to memory access)</li>
</ul>
<p><img src="/spec_decode_images/image1.png"
	
	
	
	loading="lazy"
	
		alt="Performance Comparison"
	
	
></p>
<p>The LLM decoding operation is <strong>memory-bound</strong> as described above, and hence making compute units faster will not make the inference faster proportionally. Any truly effective acceleration strategy for the decoding phase must find a way to reduce the number of these costly memory-transfer cycles required per generated token. Hence, we would need to devise a way to parallelize an inherently sequential process.</p>
<h2 id="the-solution">The Solution
</h2><p><strong>Speculative Decoding (SD)</strong> does exactly that - it speeds up the sequential decoding by generating multiple tokens for roughly the cost of one target model memory load cycle, and as a result, it significantly improves the efficiency of the decoding process.</p>
<p>It uses a <strong>draft then verify approach</strong>:</p>
<ul>
<li><strong>Drafting</strong> produces multiple next tokens using a faster/smaller model</li>
<li><strong>Verification</strong> checks these tokens in parallel in the target/large LM</li>
</ul>
<p>The key players:</p>
<ul>
<li><strong>The Target Model (Mq‚Äã)</strong>: This is the large, powerful, and accurate LLM whose output distribution we want to replicate exactly, but is very slow.</li>
<li><strong>The Draft Model (Mp‚Äã)</strong>: This is a significantly smaller, and therefore much faster, language model or n-gram model. It might be a smaller version from the same model family, a distilled version, or even a different architecture altogether. Its role is not to be perfectly accurate, but to quickly generate plausible candidate tokens (the &ldquo;draft&rdquo;).</li>
</ul>
<p>The core idea relies on two key observations:</p>
<ul>
<li>
<p><strong>Many tokens are &ldquo;easy&rdquo;</strong>: Not all tokens in a generated response need the full power of a huge LLM - common phrases, grammatical structures, or repetitive sequences can be correctly generated by a much smaller draft model with high confidence. The large model must only be used for more nuanced generation tasks such as reasoning or factual recall - the &ldquo;hard&rdquo; tokens.</p>
</li>
<li>
<p><strong>Parallel Verification is Cheap</strong>: While generating one token with the large target model Mq‚Äã is slow due to the memory bandwidth bottleneck, verifying multiple candidate tokens (say, K draft tokens plus the original context) in a single, parallel forward pass through Mq‚Äã takes roughly the same amount of time. This is because the dominant cost ‚Äì loading the model weights ‚Äì happens only once for the entire verification batch.</p>
</li>
</ul>
<p>Speculative decoding leverages these observations. If the draft model&rsquo;s predictions align well with what the target model would have generated, multiple tokens can be accepted in a single step, effectively amortizing the cost of the expensive target model inference over several tokens.</p>
<p><img src="/spec_decode_images/image2.png"
	
	
	
	loading="lazy"
	
		alt="Draft vs Target Model"
	
	
></p>
<p>The drafting part is straightforward - you just generate K tokens from the draft model auto-regressively. The verification part, intuitively works for transformers because it bypasses the causal dependency in auto-regressive decoding by computing the logits of a sequence at once. Let&rsquo;s look at some familiar code to clarify -</p>
<h3 id="auto-regressive-decoding-single-token">Auto-Regressive Decoding (Single Token)
</h3><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Setup model and tokenizer</span>
</span></span><span class="line"><span class="cl"><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&#34;gpt2&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Prepare input context</span>
</span></span><span class="line"><span class="cl"><span class="n">context</span> <span class="o">=</span> <span class="s2">&#34;The cat sat&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Generate single token prediction</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># Logits for the next token</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="speculative-decoding-draft--verify">Speculative Decoding (Draft + Verify)
</h3><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 1: Draft Model - Generate candidate tokens</span>
</span></span><span class="line"><span class="cl"><span class="n">drafted_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;on&#34;</span><span class="p">,</span> <span class="s2">&#34;the&#34;</span><span class="p">,</span> <span class="s2">&#34;mat&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">drafted_sequence</span> <span class="o">=</span> <span class="n">context</span> <span class="o">+</span> <span class="s2">&#34; &#34;</span> <span class="o">+</span> <span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">drafted_tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 2: Prepare full sequence for verification</span>
</span></span><span class="line"><span class="cl"><span class="n">full_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">drafted_sequence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 3: Verify with target model (single forward pass)</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">full_input_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span> <span class="c1"># Shape: [batch_size, seq_len, vocab_size]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 4: Extract verification probabilities for each drafted token</span>
</span></span><span class="line"><span class="cl"><span class="n">draft_start</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># First drafted token position</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">drafted_tokens</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">position</span> <span class="o">=</span> <span class="n">draft_start</span> <span class="o">+</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># logits at position predict next token</span>
</span></span><span class="line"><span class="cl">    <span class="n">token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Log probability the verifier would assign to the draft token</span>
</span></span><span class="line"><span class="cl">    <span class="n">prob</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="n">token_id</span><span class="p">]</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Verifier probability for &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">prob</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>As you can see, we can directly get the assigned probabilities for each token at once, and hence compute the overall probability of the sequence in a single pass.</p>
<p>The last step is the <strong>acceptance/rejection token-by-token verification</strong> that decides which tokens to keep or reject from the draft - we simply follow the above idea, where we have the verifier probability of each drafted token, and check if it is the highest top token according to the verifier logits. This is the simplest acceptance/rejection method, called <strong>top-1 verification</strong>. In practice, we would use a generalized speculative decoding algorithm with Metropolis-Hastings-style acceptance criteria (often called &ldquo;adaptive speculative decoding&rdquo; or &ldquo;accept/reject sampling&rdquo;).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Top-1 Verification Implementation</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Setup for token verification</span>
</span></span><span class="line"><span class="cl"><span class="n">accepted_tokens</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">draft_start</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># First position where drafts start</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 1: Iterate through each drafted token for verification</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">draft_tokens</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Position in logits where the verifier predicts the NEXT token</span>
</span></span><span class="line"><span class="cl">    <span class="n">position</span> <span class="o">=</span> <span class="n">draft_start</span> <span class="o">+</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># position -1 because logits predict the next token</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Step 2: Get verifier&#39;s top prediction</span>
</span></span><span class="line"><span class="cl">    <span class="n">verifier_top_token_id</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">verifier_top_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">([</span><span class="n">verifier_top_token_id</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Step 3: Get drafted token ID for comparison</span>
</span></span><span class="line"><span class="cl">    <span class="n">drafted_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Step 4: Accept/reject based on exact match with top prediction</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">verifier_top_token_id</span> <span class="o">==</span> <span class="n">drafted_token_id</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">accepted_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;ACCEPTED token &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39; at position </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;REJECTED token &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39; at position </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">break</span> <span class="c1"># Stop at first mismatch</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="rejection-sampling-approach">Rejection Sampling Approach
</h3><p>In this, instead of just checking &ldquo;is the token the same as verifier&rsquo;s top prediction?&rdquo; you give some probability of accepting even if the draft and verifier disagree - depending on how much they disagree (probabilistic acceptance).</p>
<h4 id="metropolis-hastings-acceptance-criteria">Metropolis-Hastings Acceptance Criteria
</h4><ol>
<li>Pick a random number &ldquo;r&rdquo; from uniform distribution [0,1]</li>
<li>For each token, get draft logit/probability &ldquo;p_t&rdquo; and the verifier logit/probability &ldquo;q_t&rdquo;</li>
<li>Accept draft token if r &lt; min(1, p_t/q_t) - essentially if the draft probability is much lower than the verifier probability, the chances of acceptance drop. This is where we see how much the models agree with each other.</li>
</ol>
<p>The following code illustrates the approach, along with how we then correct the remaining draft tokens if a token is rejected before reaching the end of current draft tokens.</p>
<p>The correction happens by sampling from the <strong>residual distribution</strong> max(0, (q(x) - p(x))). Intuitively, we want to do this to ensure that we ignore all the remaining token choices where the draft model logits &gt; verifier logits.</p>
<p>For example, if -</p>
<p>Draft model p(x) says:</p>
<ul>
<li>&ldquo;cat&rdquo;: 0.4</li>
<li>&ldquo;dog&rdquo;: 0.3</li>
<li>&ldquo;mouse&rdquo;: 0.3</li>
</ul>
<p>Verifier q(x) says:</p>
<ul>
<li>&ldquo;cat&rdquo;: 0.2</li>
<li>&ldquo;dog&rdquo;: 0.5</li>
<li>&ldquo;mouse&rdquo;: 0.3</li>
</ul>
<p>Draft model picked &ldquo;cat&rdquo;, but it got rejected. Now, we calculate residuals:</p>
<ul>
<li>residual(cat) = max(0, 0.2 - 0.4) = 0</li>
<li>residual(dog) = max(0, 0.5 - 0.3) = 0.2</li>
<li>residual(mouse) = max(0, 0.3 - 0.3) = 0</li>
</ul>
<p>In this case, &ldquo;dog&rdquo; remains the only option. If you just pick from q(x) again, you might resample a token the draft model already explored and proposed, wasting compute and possibly biasing the results. There is an edge case, where the residual might become all &lt; 0, in which case the algorithm must fallback to sampling from q(x) - I have no remaining unaccounted-for probability, so I&rsquo;ll just sample from the verifier&rsquo;s full distribution as if no draft proposal happened.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Setup for Metropolis-Hastings acceptance criteria</span>
</span></span><span class="line"><span class="cl"><span class="n">accepted_tokens</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">draft_start</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 1: Process drafted tokens with probabilistic acceptance</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">draft_tokens</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">position</span> <span class="o">=</span> <span class="n">draft_start</span> <span class="o">+</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Step 2: Get probability distributions from both models</span>
</span></span><span class="line"><span class="cl">    <span class="n">verifier_probs</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">draft_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Extract specific token probabilities</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_t</span> <span class="o">=</span> <span class="n">verifier_probs</span><span class="p">[</span><span class="n">draft_token_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># verifier prob</span>
</span></span><span class="line"><span class="cl">    <span class="n">p_t</span> <span class="o">=</span> <span class="n">draft_probs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="c1"># you must get this from the draft model when proposing tokens</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Step 3: Apply Metropolis-Hastings acceptance rule</span>
</span></span><span class="line"><span class="cl">    <span class="n">r</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">acceptance_threshold</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_t</span> <span class="o">/</span> <span class="n">q_t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">acceptance_threshold</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">accepted_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;ACCEPTED token &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39; with r=</span><span class="si">{</span><span class="n">r</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, threshold=</span><span class="si">{</span><span class="n">acceptance_threshold</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;REJECTED token &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39; with r=</span><span class="si">{</span><span class="n">r</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, threshold=</span><span class="si">{</span><span class="n">acceptance_threshold</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">break</span> <span class="c1"># Stop at first rejection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 4: Handle rejection with residual sampling</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">accepted_tokens</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">draft_tokens</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">position</span> <span class="o">=</span> <span class="n">draft_start</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">accepted_tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Calculate and normalize residual distribution</span>
</span></span><span class="line"><span class="cl">    <span class="n">residual</span> <span class="o">=</span> <span class="p">(</span><span class="n">verifier_probs</span> <span class="o">-</span> <span class="n">draft_probs_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">residual</span> <span class="o">/=</span> <span class="n">residual</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="c1"># Normalize</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Sample from residual distribution</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">residual</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">([</span><span class="n">next_token_id</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">accepted_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Final accepted sequence:&#34;</span><span class="p">,</span> <span class="n">accepted_tokens</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="bonus-token">Bonus Token
</h2><p>If all the draft model tokens are accepted, indicating that the draft and target model are aligned, we can actually sample an additional token from the target model since we are already generating K logits from the model, giving us K+1 tokens - this gives us a <strong>bonus token for free</strong>! Further maximizing efficiency.</p>
<h2 id="kv-cache-efficiency">KV Cache Efficiency
</h2><p>A critical aspect for efficiency, especially during the parallel verification step, is managing the <strong>Key-Value (KV) cache</strong>. Standard autoregressive decoding maintains a cache of activations from previous steps to avoid recomputing them. In speculative decoding, the target model processes multiple potential future states simultaneously. A naive implementation might require replicating the KV cache for each potential path, leading to excessive memory usage.</p>
<p>Optimized inference engines address this challenge. Techniques like <strong>PagedAttention</strong> allow sharing parts of the KV cache between different sequences in the verification batch. This works similarly to virtual memory paging in operating systems, dividing the cache into blocks that can be shared and managed efficiently, preventing memory bloat and maintaining throughput even at larger batch sizes.</p>
<h2 id="choosing-draft-and-target-models">Choosing Draft and Target Models
</h2><p>Usually draft models can be:</p>
<ul>
<li>
<p><strong>A smaller model in the same LLM family</strong> (Gemma 2B -&gt; Gemma 9B). Additionally, the smaller models can be further aligned through knowledge distillation.</p>
</li>
<li>
<p><strong>N-grams</strong>: Proposals are drawn by matching up to a chosen size n-grams from the prompt itself, effectively reusing previously seen text patterns.</p>
</li>
<li>
<p><strong>Custom MLP‚Äêbased speculator networks</strong> condition on both the context vectors and sampled tokens to predict future tokens, enabling learned proposal distributions beyond simple draft‚Äêmodel outputs.</p>
</li>
<li>
<p><strong>EAGLE</strong> (Extrapolation Algorithm for Greater Language‚Äêmodel Efficiency) draft models to extrapolate continuations, combining algorithmic lookahead with draft‚Äêmodel verification for efficient, lossless sampling.</p>
</li>
</ul>
<h2 id="key-hyperparameters">Key Hyperparameters
</h2><ul>
<li>
<p><strong>K (number of draft tokens)</strong>:</p>
<ul>
<li>Number of tokens proposed by the draft model per iteration</li>
<li>Important because it affects the balance of more accepted tokens vs. higher draft cost &amp; rejection probability</li>
<li>Optimal often 3-5. Depends on draft speed/accuracy</li>
</ul>
</li>
<li>
<p><strong>Draft Model Size/Arch</strong>:</p>
<ul>
<li>Choice of the smaller model (parameters, architecture)</li>
<li>Directly impacts draft latency (critical) and baseline acceptance rate</li>
<li>Aim for low latency. 10-20x smaller than target or use specialized architectures</li>
</ul>
</li>
<li>
<p><strong>Draft Model Alignment</strong>:</p>
<ul>
<li>How well the draft model predicts the target model&rsquo;s distribution</li>
<li>Primary driver of acceptance rate. Higher alignment = higher acceptance = more speedup</li>
<li>Use models from same family, or apply Knowledge distillation</li>
</ul>
</li>
<li>
<p><strong>Temperature</strong>:</p>
<ul>
<li>Controls randomness during sampling</li>
<li>Higher temp -&gt; lower predictability -&gt; lower acceptance rate -&gt; lower speedup</li>
<li>Performance often peaks at low/mid temps. Align KD temp with inference temp</li>
</ul>
</li>
</ul>
<p><img src="/spec_decode_images/image3.png"
	
	
	
	loading="lazy"
	
		alt="Speculative Decoding Process"
	
	
></p>
<h2 id="references">References
</h2><ul>
<li>Leviathan, Y., Kalman, M., &amp; Matias, Y. (2023). <strong>Fast Inference from Transformers via Speculative Decoding</strong>. ICML.</li>
<li>Chen, X., et al. (2023). <strong>Accelerating Large Language Model Decoding with Speculative Sampling</strong>. arXiv preprint.</li>
<li>Spector, B., &amp; Murray, K. (2023). <strong>Accelerating LLM Inference with Staged Speculative Decoding</strong>. NeurIPS.</li>
</ul>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/llm/">LLM</a>
        
            <a href="/tags/speculative-decoding/">Speculative Decoding</a>
        
            <a href="/tags/transformers/">Transformers</a>
        
            <a href="/tags/inference-optimization/">Inference Optimization</a>
        
    </section>


    </footer>


    
</article>

    

    

     
    
        
    

     

    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.3ccd51f268c21b352d6ec72a2b5c47ae3b279ba2976d1a02748766e6eb0478b2.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
